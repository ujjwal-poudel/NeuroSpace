"""
This module implements a minimal volumetric ray marching renderer for use with
a SIREN-based neural radiance field generator. The renderer takes camera rays
generated by the Camera class and samples multiple points along each ray.
Each sampled 3D point is passed into the neural field generator, which returns
color and density values. The renderer then integrates these values along each
ray using alpha compositing to produce the final 2D image.

This file provides:
1. A function to uniformly sample 3D points along a batch of rays.
2. A function to perform volume rendering using NeRF-style alpha blending.
3. A high-level render function that outputs the final image tensor.
"""

import torch
import torch.nn.functional as F


def sample_points_on_rays(rays_o, rays_d, num_samples, near=2.0, far=6.0):
    """
    Sample evenly spaced 3D points along each ray.

    Parameters:
        rays_o: Ray origins of shape (N, 3).
        rays_d: Ray direction vectors of shape (N, 3).
        num_samples: Number of sample points along each ray.
        near: Starting distance along each ray.
        far: Ending distance along each ray.

    Returns:
        pts: Sampled 3D points of shape (N, num_samples, 3).
        z_vals: Depth values for each sample, shape (N, num_samples).
    """

    N = rays_o.shape[0]

    # Create linearly spaced depth values along the ray.
    z_vals = torch.linspace(near, far, num_samples, device=rays_o.device)
    z_vals = z_vals.unsqueeze(0).expand(N, num_samples)

    # Expand ray origins and directions for vectorized computation.
    rays_o_exp = rays_o.unsqueeze(1).expand(N, num_samples, 3)
    rays_d_exp = rays_d.unsqueeze(1).expand(N, num_samples, 3)

    # Compute actual 3D sample points.
    pts = rays_o_exp + rays_d_exp * z_vals.unsqueeze(-1)

    return pts, z_vals


def volume_render(rgb, sigma, z_vals):
    """
    Perform NeRF-style volume rendering using alpha compositing.

    Parameters:
        rgb: Predicted colors of shape (N, num_samples, 3).
        sigma: Predicted densities of shape (N, num_samples, 1).
        z_vals: Depth values of shape (N, num_samples).

    Returns:
        final_rgb: Rendered RGB image of shape (N, 3).
    """

    # Compute distances between consecutive samples.
    dists = z_vals[:, 1:] - z_vals[:, :-1]
    last_dist = torch.full_like(dists[:, :1], 1e-3)
    dists = torch.cat([dists, last_dist], dim=-1)

    # Compute alpha from density.
    alpha = 1.0 - torch.exp(-sigma.squeeze(-1) * dists)

    # Compute accumulated transmittance.
    transmittance = torch.cumprod(torch.cat([
        torch.ones_like(alpha[:, :1]),
        1.0 - alpha + 1e-10
    ], dim=-1), dim=-1)[:, :-1]

    # Weight of each sample point.
    weights = alpha * transmittance

    # Compute final pixel color.
    final_rgb = torch.sum(weights.unsqueeze(-1) * rgb, dim=1)

    return final_rgb


def render_image(generator, rays_o, rays_d, H, W, num_samples=32):
    """
    Render a 2D image from a neural field generator using volumetric ray marching.

    Parameters:
        generator: Neural field model that maps 3D points to (rgb, density).
        rays_o: Ray origins of shape (H*W, 3).
        rays_d: Ray directions of shape (H*W, 3).
        H: Image height in pixels.
        W: Image width in pixels.
        num_samples: Number of samples per ray.

    Returns:
        image: A rendered image tensor of shape (H, W, 3).
    """

    # Sample points along rays.
    pts, z_vals = sample_points_on_rays(
        rays_o, rays_d,
        num_samples=num_samples
    )

    # Flatten points for batch processing through the generator.
    N, S, _ = pts.shape
    pts_flat = pts.reshape(N * S, 3)

    # Query the neural field.
    rgb_flat, sigma_flat = generator(pts_flat)

    # Reshape back to (N, S, 3) and (N, S, 1).
    rgb = rgb_flat.reshape(N, S, 3)
    sigma = sigma_flat.reshape(N, S, 1)

    # Perform volume rendering.
    final_rgb = volume_render(rgb, sigma, z_vals)

    # Reshape into the final image.
    image = final_rgb.reshape(H, W, 3)

    return image